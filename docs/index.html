<html>
  <head>
    <title>vrmc</title>
    <style>
      body {
        margin: 0;
        background-color: black;
        display: flex;
        align-items: center;
        justify-content: center;
      }

      video {
        display: none;
      }
    </style>
  </head>
  <body>
    <script type="importmap">
      {
        "imports": {
          "three": "https://cdn.jsdelivr.net/npm/three@0.166.1/build/three.module.js",
          "three/addons/": "https://cdn.jsdelivr.net/npm/three@0.166.1/examples/jsm/",
          "@pixiv/three-vrm": "https://cdn.jsdelivr.net/npm/@pixiv/three-vrm@2.1.2/+esm",
          "@mediapipe/tasks-vision": "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.14/+esm"
        }
      }
    </script>
    <script type="module">
      import * as THREE from "three";
      import { GLTFLoader } from "three/addons/loaders/GLTFLoader.js";
      import { OrbitControls } from "three/addons/controls/OrbitControls.js";
      import { VRMLoaderPlugin, VRMUtils } from "@pixiv/three-vrm";
      import { FilesetResolver, FaceLandmarker } from "@mediapipe/tasks-vision";

      // renderer
      const renderer = new THREE.WebGLRenderer();
      renderer.setSize(window.innerWidth, window.innerHeight);
      renderer.setPixelRatio(window.devicePixelRatio);
      document.body.appendChild(renderer.domElement);

      // camera
      const camera = new THREE.PerspectiveCamera(
        30.0,
        window.innerWidth / window.innerHeight,
        0.1,
        20.0,
      );
      camera.position.set(0.0, 1.35, 1.3);

      // camera controls
      const controls = new OrbitControls(camera, renderer.domElement);
      let orbitTarget = camera.position.clone();
      orbitTarget.z -= 5;
      controls.target = orbitTarget;
      controls.update();

      // scene
      const scene = new THREE.Scene();
      scene.background = new THREE.Color(0xff00ff);

      // light
      const light = new THREE.DirectionalLight(0xffffff, Math.PI);
      light.position.set(1.0, 1.0, 1.0).normalize();
      scene.add(light);

      // gltf and vrm
      let currentVrm = undefined;

      let lookAtTarget = new THREE.Object3D();
      lookAtTarget.position.z = 10;
      scene.add(lookAtTarget);

      const loader = new GLTFLoader();
      loader.crossOrigin = "anonymous";

      loader.register((parser) => {
        return new VRMLoaderPlugin(parser);
      });

      const load = (url) => {
        loader.load(
          url,

          (gltf) => {
            const vrm = gltf.userData.vrm;

            // calling these functions greatly improves the performance
            VRMUtils.removeUnnecessaryVertices(gltf.scene);
            VRMUtils.removeUnnecessaryJoints(gltf.scene);

            if (currentVrm) {
              scene.remove(currentVrm.scene);
              VRMUtils.deepDispose(currentVrm.scene);
            }

            // Disable frustum culling
            vrm.scene.traverse((obj) => {
              obj.frustumCulled = false;
            });

            adjustArmsToIPose(vrm);
            vrm.lookAt.target = lookAtTarget;
            vrm.update(0);

            currentVrm = vrm;
            scene.add(vrm.scene);

            console.log(vrm);
          },

          (progress) =>
            console.log(
              "Loading model...",
              100.0 * (progress.loaded / progress.total),
              "%",
            ),

          (error) => console.error(error),
        );
      };

      load("./models/VRM1_Constraint_Twist_Sample.vrm");

      // Adjust arms to I-pose function
      const adjustArmsToIPose = (vrm) => {
        const leftUpperArm = vrm.humanoid.getNormalizedBoneNode("leftUpperArm");
        const rightUpperArm =
          vrm.humanoid.getNormalizedBoneNode("rightUpperArm");
        if (leftUpperArm) {
          leftUpperArm.rotation.z = -Math.PI / 2.5; // Rotate left arm down
        }
        if (rightUpperArm) {
          rightUpperArm.rotation.z = Math.PI / 2.5; // Rotate right arm down
        }
      };

      // animate
      const clock = new THREE.Clock();
      clock.start();

      const animate = () => {
        requestAnimationFrame(animate);

        // render
        renderer.render(scene, camera);
      };

      let faceLandmarker;
      let video;

      const detectFaceLandmarks = (time) => {
        if (!faceLandmarker) {
          return;
        }
        const landmarks = faceLandmarker.detectForVideo(video, time);

        // Apply transformation
        const transformationMatrices = landmarks.facialTransformationMatrixes;
        if (transformationMatrices && transformationMatrices.length > 0) {
          const matrix = new THREE.Matrix4().fromArray(
            transformationMatrices[0].data,
          );
          const originalQuaternion =
            new THREE.Quaternion().setFromRotationMatrix(matrix);

          const reflectQuaternion = new THREE.Quaternion(
            originalQuaternion.x > 0
              ? originalQuaternion.x
              : originalQuaternion.x * 0.3,
            -originalQuaternion.y * 0.5,
            -originalQuaternion.z,
            originalQuaternion.w,
          );

          if (currentVrm) {
            const spineBone =
              currentVrm.humanoid.getNormalizedBoneNode("spine");
            const neckBone = currentVrm.humanoid.getNormalizedBoneNode("neck");

            spineBone.quaternion.slerp(
              adjustQuaternionAngleRatio(reflectQuaternion, 0.6),
              0.3,
            );
            neckBone.quaternion
              .copy(spineBone.quaternion)
              .multiply(adjustQuaternionAngleRatio(reflectQuaternion, 0.2));
          }
        }

        // Apply Blendshapes
        const blendshapes = landmarks.faceBlendshapes;
        if (blendshapes && blendshapes.length > 0) {
          const coefsMap = retarget(blendshapes);
          updateBlendshapes(coefsMap);
        }
      };

      const adjustQuaternionAngleRatio = (quaternion, ratio) => {
        return new THREE.Quaternion(
          quaternion.x * ratio,
          quaternion.y * ratio,
          quaternion.z * ratio,
          quaternion.w,
        );
      };

      const updateBlendshapes = (blendshapes) => {
        updateFacial(blendshapes);

        const deltaTime = clock.getDelta();

        currentVrm.update(deltaTime);
      };

      const updateFacial = (blendshapes) => {
        if (!currentVrm) {
          return;
        }

        const blink = Math.max(
          blendshapes.get("eyeBlinkLeft"),
          blendshapes.get("eyeBlinkRight"),
        );
        currentVrm.expressionManager.setValue("blinkLeft", (blink - 0.5) * 2);
        currentVrm.expressionManager.setValue("blinkRight", (blink - 0.5) * 2);

        const jawOpen = blendshapes.get("jawOpen");
        if (jawOpen > 0.3) {
          currentVrm.expressionManager.setValue("ee", 0);
          currentVrm.expressionManager.setValue("aa", jawOpen);
        } else if (jawOpen > 0.2) {
          currentVrm.expressionManager.setValue("aa", 0);
          currentVrm.expressionManager.setValue("ee", jawOpen);
        } else {
          currentVrm.expressionManager.setValue("aa", 0);
          currentVrm.expressionManager.setValue("ee", 0);
        }

        const mouthPucker = blendshapes.get("mouthPucker");
        const ou = (mouthPucker - 0.5) * 2;
        if (ou > 0) {
          currentVrm.expressionManager.setValue("ou", ou);
          currentVrm.expressionManager.setValue("ee", 0);
          currentVrm.expressionManager.setValue("aa", 0);
        } else {
          currentVrm.expressionManager.setValue("ou", 0);
        }

        const mouthFunnel = blendshapes.get("mouthFunnel");
        if (mouthFunnel > 0.2) {
          currentVrm.expressionManager.setValue("oh", mouthFunnel);
        } else {
          currentVrm.expressionManager.setValue("oh", 0);
        }

        const browInnerUp = blendshapes.get("browInnerUp");
        if (browInnerUp > 0.7) {
          currentVrm.expressionManager.setValue("surprised", browInnerUp);
        } else {
          currentVrm.expressionManager.setValue("surprised", 0);
        }

        const mouthSmileRight = blendshapes.get("mouthSmileRight");
        const mouthSmileLeft = blendshapes.get("mouthSmileLeft");
        if ((mouthSmileRight || 0) + (mouthSmileLeft || 0) > 0.5) {
          currentVrm.expressionManager.setValue("blinkLeft", 0);
          currentVrm.expressionManager.setValue("blinkRight", 0);
          currentVrm.expressionManager.setValue(
            "happy",
            mouthSmileRight + mouthSmileLeft,
          );
          currentVrm.expressionManager.setValue("surprised", 0);
        } else {
          currentVrm.expressionManager.setValue("happy", 0);
        }

        const rightEyeLookRight =
          (blendshapes.get("eyeLookOutRight") || 0) -
          (blendshapes.get("eyeLookInRight") || 0);
        const leftEyeLookRight =
          (blendshapes.get("eyeLookInLeft") || 0) -
          (blendshapes.get("eyeLookOutLeft") || 0);

        const rightEyeLookUp =
          (blendshapes.get("eyeLookUpRight") || 0) -
          (blendshapes.get("eyeLookDownRight") || 0);
        const leftEyeLookUp =
          (blendshapes.get("eyeLookUpLeft") || 0) -
          (blendshapes.get("eyeLookDownLeft") || 0);

        lookAtTarget.position.x =
          (10 * (rightEyeLookRight + leftEyeLookRight)) / 2;
        lookAtTarget.position.y =
          1.7 + (10 * (rightEyeLookUp + leftEyeLookUp)) / 2;
      };

      const retarget = (blendshapes) => {
        const categories = blendshapes[0].categories;
        let coefsMap = new Map();
        categories.forEach((blendshape) => {
          // Adjust certain blendshape values to be less prominent.
          switch (blendshape.categoryName) {
            case "eyeBlinkLeft":
              blendshape.score *= 1.2;
              break;
            case "eyeBlinkRight":
              blendshape.score *= 1.2;
              break;
            default:
          }
          coefsMap.set(blendshape.categoryName, blendshape.score);
        });

        return coefsMap;
      };

      const onVideoFrame = (time) => {
        // Do something with the frame.
        detectFaceLandmarks(time);
        // Re-register the callback to be notified about the next frame.
        video.requestVideoFrameCallback(onVideoFrame);
      };

      // Stream webcam into landmarker loop (and also make video visible)
      const streamWebcamThroughFaceLandmarker = async () => {
        video = document.getElementById("video");

        const onAcquiredUserMedia = (stream) => {
          video.srcObject = stream;
          video.onloadedmetadata = () => {
            video.play();
          };
        };

        try {
          const evt = await navigator.mediaDevices.getUserMedia({
            audio: false,
            video: {
              facingMode: "user",
              width: 1280,
              height: 720,
            },
          });
          onAcquiredUserMedia(evt);
          video.requestVideoFrameCallback(onVideoFrame);
        } catch (e) {
          console.error(`Failed to acquire camera feed: ${e}`);
        }
      };

      const loadMediapipeModel = async () => {
        await streamWebcamThroughFaceLandmarker();
        const vision = await FilesetResolver.forVisionTasks(
          "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.14/wasm",
        );
        faceLandmarker = await FaceLandmarker.createFromModelPath(
          vision,
          "https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/latest/face_landmarker.task",
        );
        await faceLandmarker.setOptions({
          baseOptions: {
            delegate: "GPU",
          },
          runningMode: "VIDEO",
          outputFaceBlendshapes: true,
          outputFacialTransformationMatrixes: true,
        });

        console.log("Finished Loading MediaPipe Model.");
      };

      animate();
      loadMediapipeModel();

      // dnd handler
      window.addEventListener("dragover", function (event) {
        event.preventDefault();
      });

      window.addEventListener("drop", function (event) {
        event.preventDefault();

        // read given file then convert it to blob url
        const files = event.dataTransfer.files;
        if (!files) {
          return;
        }

        const file = files[0];
        if (!file) {
          return;
        }

        const blob = new Blob([file], { type: "application/octet-stream" });
        const url = URL.createObjectURL(blob);
        load(url);
      });
    </script>

    <video autoplay playsinline id="video"><track kind="captions" /></video>
  </body>
</html>
