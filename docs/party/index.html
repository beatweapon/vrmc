<!DOCTYPE html>
<html lang="ja">
  <head>
    <meta charset="UTF-8" />
    <title>VRM Sync Test</title>
    <style>
      body {
        margin: 0;
        background: #000;
        overflow: hidden;
      }
      canvas {
        display: block;
      }
      #video {
        display: none;
      }
    </style>

    <!-- Socket.IO -->
    <script src="https://cdn.socket.io/4.8.1/socket.io.min.js"></script>
  </head>
  <body>
    <script type="importmap">
      {
        "imports": {
          "three": "https://cdn.jsdelivr.net/npm/three@0.177.0/build/three.module.js",
          "three/addons/": "https://cdn.jsdelivr.net/npm/three@0.177.0/examples/jsm/",
          "three/webgpu": "https://cdn.jsdelivr.net/npm/three@0.177.0/build/three.webgpu.js",
          "three/tsl": "https://cdn.jsdelivr.net/npm/three@0.177.0/build/three.tsl.js",
          "@pixiv/three-vrm": "https://cdn.jsdelivr.net/npm/@pixiv/three-vrm@3/lib/three-vrm.module.min.js",
          "@pixiv/three-vrm/nodes": "https://cdn.jsdelivr.net/npm/@pixiv/three-vrm@3/lib/nodes/index.module.js",
          "@mediapipe/tasks-vision": "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.22-rc.20250304/+esm"
        }
      }
    </script>
    <video id="video" autoplay playsinline></video>
    <script type="module">
      import * as THREE from "three";
      import * as vision from "@mediapipe/tasks-vision";
      import { VRMAvatar } from "../vrmAvatar.js";
      import { createIndexedDB } from "./indexedDB.js";

      const DB_NAME = "vrmc";
      const STORE_NAME = "vrm";
      const db = createIndexedDB(DB_NAME, STORE_NAME);

      // 固定ユーザーIDを取得（なければ作る）
      const getUserId = () => {
        let id = localStorage.getItem("userId");
        if (!id) {
          id = crypto.randomUUID();
          localStorage.setItem("userId", id);
        }
        return id;
      };

      const userId = getUserId();

      const { FaceLandmarker, FilesetResolver } = vision;

      const SERVER_URL = "https://63d00d9d6f23.ngrok-free.app";
      const socket = io(SERVER_URL, { transports: ["websocket"] });

      const scene = new THREE.Scene();
      const camera = new THREE.PerspectiveCamera(
        30,
        window.innerWidth / window.innerHeight,
        0.1,
        20,
      );
      camera.position.set(0, 1.4, 1.5);
      const renderer = new THREE.WebGLRenderer({ antialias: true });
      renderer.setSize(window.innerWidth, window.innerHeight);
      document.body.appendChild(renderer.domElement);

      // light
      const light = new THREE.AmbientLight(0xffffff, Math.PI);
      scene.add(light);

      const myAvatarModel = await db.loadData(userId);
      const blob = new Blob([myAvatarModel], {
        type: "application/octet-stream",
      });
      const url = URL.createObjectURL(blob);

      // VRMアバター
      const myAvatar = new VRMAvatar(
        url ? url : "../models/VRM1_Constraint_Twist_Sample.vrm",
        scene,
        { x: -0.5, y: 0, z: 0 },
      );
      const otherAvatar = new VRMAvatar(
        "../models/VRM1_Constraint_Twist_Sample.vrm",
        scene,
        { x: 0.5, y: 0, z: 0 },
      );

      // ドラッグ＆ドロップ対応
      document.addEventListener("dragover", (e) => e.preventDefault());
      document.addEventListener("drop", async (e) => {
        e.preventDefault();
        const file = e.dataTransfer.files[0];
        if (!file.name.endsWith(".vrm")) {
          alert("VRMファイルをドロップしてください");
          return;
        }

        const arrayBuffer = await file.arrayBuffer();
        console.log("圧縮中...");

        console.log("IndexedDBに保存中...");
        await db.saveData(userId, arrayBuffer);

        const blob = new Blob([arrayBuffer], {
          type: "application/octet-stream",
        });
        const url = URL.createObjectURL(blob);

        await myAvatar.changeModel(url);
      });

      // カメラ初期化
      async function initCamera() {
        const video = document.getElementById("video");
        video.srcObject = await navigator.mediaDevices.getUserMedia({
          video: true,
        });
        return new Promise((r) => (video.onloadedmetadata = () => r(video)));
      }

      // Tracking variables
      let myFaceResult = null;
      let prevOtherResult = null;
      let nextOtherResult = null;
      let interpolationStartTime = 0;
      const interpolationDuration = 0.05; // 50ms

      // Mediapipeによる顔トラッキング開始
      async function startFaceTracking(video) {
        const filesetResolver = await FilesetResolver.forVisionTasks(
          "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.22-rc.20250304/wasm",
        );
        const faceLandmarker = await FaceLandmarker.createFromOptions(
          filesetResolver,
          {
            baseOptions: {
              modelAssetPath:
                "https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/latest/face_landmarker.task",
              delegate: "GPU",
            },
            outputFaceBlendshapes: true,
            outputFacialTransformationMatrixes: true,
            numFaces: 1,
            runningMode: "VIDEO",
          },
        );

        async function detect() {
          const result = await faceLandmarker.detectForVideo(
            video,
            performance.now(),
          );

          if (result.faceLandmarks?.length > 0) {
            myFaceResult = result;
          }

          requestAnimationFrame(detect);
        }
        detect();
      }

      // WebSocket送信（30fps）
      setInterval(() => {
        if (myFaceResult) {
          socket.emit("motion", myFaceResult);
        }
      }, 33);

      // WebSocket受信
      socket.on("motion", (result) => {
        prevOtherResult = nextOtherResult
          ? structuredClone(nextOtherResult)
          : structuredClone(result);
        nextOtherResult = structuredClone(result);
        interpolationStartTime = performance.now() / 1000;
      });

      // 描画ループ
      const lerp = (a, b, t) => a + (b - a) * t;

      // Interpolation
      function interpolateResults(prev, next, alpha) {
        if (!prev || !next) return next || prev;
        const result = structuredClone(prev);

        if (prev.faceLandmarks && next.faceLandmarks) {
          for (let i = 0; i < prev.faceLandmarks[0].length; i++) {
            result.faceLandmarks[0][i].x = lerp(
              prev.faceLandmarks[0][i].x,
              next.faceLandmarks[0][i].x,
              alpha,
            );
            result.faceLandmarks[0][i].y = lerp(
              prev.faceLandmarks[0][i].y,
              next.faceLandmarks[0][i].y,
              alpha,
            );
            result.faceLandmarks[0][i].z = lerp(
              prev.faceLandmarks[0][i].z,
              next.faceLandmarks[0][i].z,
              alpha,
            );
          }
        }

        if (prev.faceBlendshapes && next.faceBlendshapes) {
          const prevCats = prev.faceBlendshapes[0].categories;
          const nextCats = next.faceBlendshapes[0].categories;
          for (let i = 0; i < prevCats.length; i++) {
            result.faceBlendshapes[0].categories[i].score = lerp(
              prevCats[i].score,
              nextCats[i].score,
              alpha,
            );
          }
        }

        return result;
      }

      const clock = new THREE.Clock();
      function animate() {
        requestAnimationFrame(animate);

        const delta = clock.getDelta();

        // Apply my face result
        if (myFaceResult) myAvatar.updateBlendshapes(myFaceResult);

        // Apply other face result (with interpolation)
        if (prevOtherResult && nextOtherResult) {
          const now = performance.now() / 1000;
          const alpha = Math.min(
            (now - interpolationStartTime) / interpolationDuration,
            1,
          );
          const frameData =
            alpha >= 1
              ? nextOtherResult
              : interpolateResults(prevOtherResult, nextOtherResult, alpha);
          otherAvatar.updateBlendshapes(frameData);
        }

        // Physics / LookAt update (once per frame for all avatars)
        if (myAvatar.vrm) myAvatar.vrm.update(delta);
        if (otherAvatar.vrm) otherAvatar.vrm.update(delta);

        renderer.render(scene, camera);
      }

      // 実行
      initCamera().then((v) => startFaceTracking(v));
      animate();
    </script>
  </body>
</html>
