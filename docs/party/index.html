<!DOCTYPE html>
<html lang="ja">
  <head>
    <meta charset="UTF-8" />
    <title>VRM Sync Test</title>
    <style>
      body {
        margin: 0;
        background: #000;
        overflow: hidden;
      }
      canvas {
        display: block;
      }
      #video {
        display: none;
      }
    </style>

    <!-- Socket.IO -->
    <script src="https://cdn.socket.io/4.8.1/socket.io.min.js"></script>
  </head>
  <body>
    <script type="importmap">
      {
        "imports": {
          "three": "https://cdn.jsdelivr.net/npm/three@0.177.0/build/three.module.js",
          "three/addons/": "https://cdn.jsdelivr.net/npm/three@0.177.0/examples/jsm/",
          "three/webgpu": "https://cdn.jsdelivr.net/npm/three@0.177.0/build/three.webgpu.js",
          "three/tsl": "https://cdn.jsdelivr.net/npm/three@0.177.0/build/three.tsl.js",
          "@pixiv/three-vrm": "https://cdn.jsdelivr.net/npm/@pixiv/three-vrm@3/lib/three-vrm.module.min.js",
          "@pixiv/three-vrm/nodes": "https://cdn.jsdelivr.net/npm/@pixiv/three-vrm@3/lib/nodes/index.module.js",
          "@mediapipe/tasks-vision": "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.22-rc.20250304/+esm"
        }
      }
    </script>
    <video id="video" autoplay playsinline></video>
    <script type="module">
      import * as THREE from "three";
      import * as vision from "@mediapipe/tasks-vision";
      import { VRMAvatar } from "../vrmAvater.js";

      const { FaceLandmarker, FilesetResolver } = vision;

      const SERVER_URL = "https://63d00d9d6f23.ngrok-free.app";
      const socket = io(SERVER_URL, { transports: ["websocket"] });

      const scene = new THREE.Scene();
      const camera = new THREE.PerspectiveCamera(
        30,
        window.innerWidth / window.innerHeight,
        0.1,
        20,
      );
      camera.position.set(0, 1.4, 1.5);
      const renderer = new THREE.WebGLRenderer({ antialias: true });
      renderer.setSize(window.innerWidth, window.innerHeight);
      document.body.appendChild(renderer.domElement);

      // light
      const light = new THREE.AmbientLight(0xffffff, Math.PI);
      light.position.set(1.0, 1.0, 1.0).normalize();
      scene.add(light);

      // VRMAvatarの利用
      const myAvatar = new VRMAvatar(
        "../models/VRM1_Constraint_Twist_Sample.vrm",
        scene,
        { x: -0.5, y: 0, z: 0 },
      );
      const otherAvatar = new VRMAvatar(
        "../models/VRM1_Constraint_Twist_Sample.vrm",
        scene,
        { x: 0.5, y: 0, z: 0 },
      );

      async function initCamera() {
        const video = document.getElementById("video");
        video.srcObject = await navigator.mediaDevices.getUserMedia({
          video: true,
        });
        return new Promise((r) => (video.onloadedmetadata = () => r(video)));
      }

      function calcHeadRotation(landmarks) {
        const left = landmarks[33];
        const right = landmarks[263];
        const nose = landmarks[1];
        const dx = right.x - left.x;
        const dy = nose.y - (left.y + right.y) / 2;
        return {
          x: dy * 2,
          y: dx * 4,
          z: 0,
        };
      }

      const clock = new THREE.Clock();
      clock.start();

      async function startFaceTracking(video) {
        const filesetResolver = await FilesetResolver.forVisionTasks(
          "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.22-rc.20250304/wasm",
        );
        const faceLandmarker = await FaceLandmarker.createFromOptions(
          filesetResolver,
          {
            baseOptions: {
              modelAssetPath:
                "https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/latest/face_landmarker.task",
              delegate: "GPU",
            },
            outputFaceBlendshapes: true,
            outputFacialTransformationMatrixes: true,
            numFaces: 1,
            runningMode: "VIDEO",
          },
        );

        async function detect() {
          const result = await faceLandmarker.detectForVideo(
            video,
            performance.now(),
          );

          console.log(result);

          if (result.faceLandmarks && result.faceLandmarks.length > 0) {
            const delta = clock.getDelta();

            myAvatar.updateBlendshapes(result, delta);

            socket.emit("motion", result);
          }
          requestAnimationFrame(detect);
        }
        detect();
      }

      socket.on("motion", (result) => {
        const delta = clock.getDelta();

        otherAvatar.updateBlendshapes(result, delta);
      });

      initCamera().then((v) => startFaceTracking(v));

      (function animate() {
        requestAnimationFrame(animate);

        renderer.render(scene, camera);
      })();
    </script>
  </body>
</html>
